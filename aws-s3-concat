#!/usr/bin/env python3
#
# Copyright 2017 Pixelworks Inc.
#
# Author: Houyu Li <hyli@pixelworks.com>
#
# This script downloads and concats splitted files from AWS S3 directly into one local file or STDOUT.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os, sys, getopt, re, traceback, json, io

import boto3

f_rw_chunksize = 1024 * 1024

def usage():
    msg_usage_vars = { "prog": sys.argv[0] }
    msg_usage = """
This script downloads and concats splitted files from AWS S3 directly into one local file or STDOUT.
During the process, each splitted file will be saved to memory first and then written to output file.
Please make sure your system has enough RAM to hold one splitted file in memory. Otherwise the script
will fail.

Example for creating splitted files by using GNU split command and streaming directly to AWS S3 like following:
$ tar -czvf - "<orig_big_file>"  |split -a 3 -b 100m - "test.tgz-" --filter 'aws s3 cp - s3://<bucket>/<prefix>$FILE'

USAGE:

{prog} -b|--bucket <bucket> -P|--file-prefix <prefix> [-o|--output-file <file>] [-p|--profile <profile>] [-h|--help]

    -b|--bucket <bucket>        : S3 bucket name
    -P|--file-prefix <prefix>   : Prefix to splitted files
    -o|--output-file <file>     : Optional. If ommitted, concated file content will be printed to STDOUT
    -p|--profile <profile>      : Optional. Use a named profile for AWS access
    -h|--help                   : Optional. Print this usage
""".format(**msg_usage_vars)
    print(msg_usage, file = sys.stderr)

def main(argv):
    # The default credential profile
    sess_profile = "default"
    # Other arguments
    bucket = ""
    file_prefix = ""
    output_file = ""
    output_to_stdout = 1

    # Dealing with input options / arguments
    try:
        opts, args = getopt.getopt(argv, "b:P:o:p:h", ["bucket=", "file-prefix=", "output-file=", "profile=", "help"])
    except getopt.GetoptError:
        usage()
        sys.exit(2)

    for opt, arg in opts:
        if opt in ("-h", "--help"):
            usage()
            sys.exit(1)
        elif opt in ("-b", "--bucket"):
            bucket = arg
        elif opt in ("-P", "--file-prefix"):
            file_prefix = arg
        elif opt in ("-o", "--output-file"):
            output_file = arg
            output_to_stdout = 0
        elif opt in ("-p", "--profile"):
            sess_profile = arg

    # Check if bucket is an empty string
    match_empty_bucket = re.match(r'^\s*$', bucket, re.M|re.I)
    if match_empty_bucket:
        usage()
        sys.exit(2)

    # Check if file_prefix is an empty string
    match_empty_file_prefix = re.match(r'^\s*$', file_prefix, re.M|re.I)
    if match_empty_file_prefix:
        usage()
        sys.exit(2)

    # Check if output_file is an empty string
    match_empty_output_file = re.match(r'^\s*$', output_file, re.M|re.I)
    if output_to_stdout != 1 and match_empty_output_file:
        usage()
        sys.exit(2)

    # Connect AWS and create group
    try:
        ## Initialize boto3 session with give credential profile
        sess = boto3.Session(profile_name = sess_profile)

        ## Create S3 client in the boto3 session
        s3_cli = sess.client("s3")

        ## Load list of target objects on S3
        target_objs = s3_cli.list_objects(
            Bucket = bucket,
            MaxKeys = 128,
            Prefix = file_prefix
        )

        ## Extract object keys to a list and sort
        file_keys = []
        file_sizes = dict()
        for target_obj in target_objs["Contents"]:
            file_keys.append(target_obj["Key"])
            file_sizes[target_obj["Key"]] = target_obj["Size"]

        ## Make sure objects are in correct order
        file_keys.sort()

        if output_to_stdout != 1:
            ## Create the output file
            f_output = open(output_file, "ab")
        else:
            ## Output to STDOUT
            f_output = sys.stdout.buffer

        ## Download S3 objects and write to output file
        for file_key in file_keys:
            print("Concating " + file_key, file = sys.stderr)
            ## Open an in memory byte stream
            buff = io.BytesIO()
            ## Download file to byte stream
            s3_cli.download_fileobj(bucket, file_key, buff)
            ## Put the pointer to beginning
            buff.seek(0)
            ## Write byte stream to output file
            buff_bytes_read = 0
            while buff_bytes_read < file_sizes[file_key]:
                f_output.write(buff.read(f_rw_chunksize))
                buff_bytes_read += f_rw_chunksize
            ## Dealing with remaining bytes
            buff_bytes_remain = file_sizes[file_key] - buff_bytes_read
            if buff_bytes_remain > 0:
                f_output.write(buff.read(buff_bytes_remain))
            ## Close byte stream
            buff.close()

        ## Close file
        if output_to_stdout != 1:
            f_output.close()
        else:
            f_output.flush()

    ## Catch all exception and print out message
    except:
        formatted_lines = traceback.format_exc().splitlines()
        print(formatted_lines[-1], file = sys.stderr)
        sys.exit(3)

if __name__ == "__main__":
    main(sys.argv[1:])
